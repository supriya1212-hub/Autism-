import numpy as np
import cv2
import autopy
from matplotlib import pyplot as plt
import pygame
import time

ESCAPE_KEY = 27

k=1
sumx=0
sumy=0
orn=7
time_engagement = 0  # Initializing time engagement

# Open the video file
cap = cv2.VideoCapture('child gesture.mp4')

screen_resolution=(1280,720)
video_resolution=(1280,720)

pygame.init()
screen = pygame.display.set_mode((600,480))

screen_resolution = autopy.screen.size()

eye_x_positions = list()
eye_y_positions = list()
gaze_start_time = time.time()

while cap.isOpened():  # Loop until video ends
    success, image = cap.read()
    if not success:
        break  # Break the loop when the video ends
    
    image = cv2.flip(image, 1)
    roi = image[150:250 , 230:330]
    resized1 = cv2.resize(roi, (200,200), interpolation=cv2.INTER_AREA)
    
    cv2.circle(resized1, (160, 127), 2, (155, 155, 255), 4)
    cv2.circle(resized1, (50, 127), 2, (155, 155, 255), 4)
    
    cv2.imshow("Par√ßa", resized1)
    
    resized1 = resized1[80:170 , 50:160] 
    resized = cv2.resize(resized1, (440,360), interpolation=cv2.INTER_AREA)
    rows, cols, _ = resized1.shape
    gray1 = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
    eye_blur = cv2.bilateralFilter(gray1,  10, 195,195)
    cv2.imshow("eye_blur", eye_blur)
    img_blur = cv2.Canny(eye_blur, 10, 30)
    
    cv2.imshow("img_blur", img_blur)
    cv2.imshow('Canny', img_blur)
    
    circles = cv2.HoughCircles(img_blur, cv2.HOUGH_GRADIENT, 0.1, 400, param1=200, param2=10, minRadius=76, maxRadius=84)
    
    current_time = time.time()
    time_difference = current_time - gaze_start_time
    
    if circles is not None:
        circles = np.uint16(np.around(circles))
        for i in circles[0, :]:
            cv2.circle(resized, (i[0], i[1]), i[2], (0, 255, 0), 2)
            cv2.circle(resized, (i[0], i[1]), 2, (0, 0, 255), 5)
            if k == orn:
                k = 1
                sumx = sumx / orn
                sumx = round(sumx, 2)
                sumy = sumy / orn
                sumy = round(sumy, 2)
                print("\n", sumx, sumy, "\n")
                eye_x_p = round((sumx - 145), 2)
                eye_y_p = round((sumy - 145), 2)
                pygame.draw.circle(screen, (0,0,255), (eye_x_p*4, eye_y_p*9), 5)
                eye_x_positions.append(eye_x_p)
                eye_y_positions.append(eye_y_p)
                print("Object gaze time:", time_difference)  # Print time engagement
                gaze_start_time = time.time()  # Reset gaze start time
                time_engagement = 0  # Reset time engagement
            elif k == 0:
                pygame.display.update()
                screen.fill((0,0,0))
                sumx = 0
                sumy = 0
                k += 1
            elif k == 1:
                pygame.display.update()
                screen.fill((0,0,0))
                sumx = sumx + i[0]
                sumy = sumy + i[1]
                k += 1
            else:
                sumx = sumx + i[0]
                sumy = sumy + i[1]
                k += 1
            cv2.putText(image, str(i[0]), (200,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (155, 255, 0), 2)
            cv2.putText(image, str(i[1]), (200,50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (155, 255, 0), 2)
    
    cv2.putText(image, "Left eye x location =", (20,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (155, 255, 0), 2)
    cv2.putText(image, "Left eye y location =", (20,50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (155, 255, 0), 2)
    cv2.imshow("Eye", resized)
    cv2.imshow("frame", image) 
    key_pressed = cv2.waitKey(1) & 0xff
    if key_pressed == ESCAPE_KEY:
        break

data_all = list(zip(eye_x_positions, eye_y_positions))
print(data_all)
plt.scatter(eye_x_positions, eye_y_positions, color="blue")
plt.title("Eye position")
plt.xlabel("X position")
plt.ylabel("Y position")
plt.axis([0, 150, 55, 0])
plt.show()

pygame.display.quit()
cap.release()
cv2.destroyAllWindows()

#emotion detection

import cv2
import numpy as np
import tensorflow as tf

# Load pre-trained model
model = tf.keras.models.load_model('model.h5')

# Define emotions (assuming the model outputs probabilities for each emotion)
emotions = ["Angry", "Disgust", "Fear", "Happy", "Sad", "Surprise", "Neutral"]

# Load video
cap = cv2.VideoCapture('child gesture.mp4')

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Perform face detection (you can use any face detection method)
    # Example: using Haar cascades
    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)

    for (x, y, w, h) in faces:
        # Extract face ROI
        face_roi = frame[y:y + h, x:x + w]
        face_roi = cv2.resize(face_roi, (48, 48))  # Assuming model expects 48x48 input

        # Normalize face ROI
        # face_roi = face_roi / 255.0
        face_roi = np.expand_dims(face_roi, axis=0)
        face_roi = np.expand_dims(face_roi, axis=-1)

        # Predict emotion
        predictions = model.predict(face_roi)
        emotion = emotions[np.argmax(predictions)]

        # Draw bounding box and label
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(frame, emotion, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    # Display frame
    cv2.imshow('Emotion Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()



   
        


